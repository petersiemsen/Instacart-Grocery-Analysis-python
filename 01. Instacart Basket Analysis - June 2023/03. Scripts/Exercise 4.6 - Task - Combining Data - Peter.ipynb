{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0538bb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0951dacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the default path\n",
    "\n",
    "path = r'C:\\Users\\peter\\Desktop\\Career Foundry - Data Analyst\\Data Immersion\\Achievement 4 - Python\\01. Instacart Basket Analysis - June 2023'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24f7d6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the product csv file\n",
    "\n",
    "df_products = pd.read_csv(os.path.join(path, '02. Data', 'Prepared Data', 'products_checked.csv'), index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2df344",
   "metadata": {},
   "source": [
    "### Step 3: Importing the new combined orders/products data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ae033ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\peter\\\\Desktop\\\\Career Foundry - Data Analyst\\\\Data Immersion\\\\Achievement 4 - Python\\\\01. Instacart Basket Analysis - June 2023\\\\02. Data\\\\Prepared Data\\\\orders_products_combined.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the orders_products_combined pkl file\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m df_orders_products_combined \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m02. Data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPrepared Data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43morders_products_combined.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py:190\u001b[0m, in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    189\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[1;32m--> 190\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    197\u001b[0m \n\u001b[0;32m    198\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[0;32m    204\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:865\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    866\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    868\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\peter\\\\Desktop\\\\Career Foundry - Data Analyst\\\\Data Immersion\\\\Achievement 4 - Python\\\\01. Instacart Basket Analysis - June 2023\\\\02. Data\\\\Prepared Data\\\\orders_products_combined.pkl'"
     ]
    }
   ],
   "source": [
    "# Importing the orders_products_combined pkl file\n",
    "\n",
    "df_orders_products_combined = pd.read_pickle(os.path.join(path, '02. Data', 'Prepared Data', 'orders_products_combined.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1808c5fd",
   "metadata": {},
   "source": [
    "### Step 4: Checking the orders/product pkl data to make sure it was imported correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab26c79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape of our pkl dataframe\n",
    "\n",
    "df_orders_products_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a8bfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking over the first 5 rows to see that we have all the columns we're expecting\n",
    "\n",
    "df_orders_products_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cb92c8",
   "metadata": {},
   "source": [
    "### Step 5: Determine a way to combine the orders_products_combined with the products dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0847692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the columns to see what we should join the two dataframes on\n",
    "\n",
    "df_products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fd267f",
   "metadata": {},
   "source": [
    "#### Both dataframes have a shared column of 'product_id'. I'm going to run a few different joins to see\n",
    "#### how many orders have a product_id associated with them. We are going for a full match, so the inner join \n",
    "#### will be the best option. I'll run the outer join just to check as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84d2d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the df_orders_products_combined dataframe with the df_products frame\n",
    "\n",
    "df_orders_products_merged = df_orders_products_combined.merge(df_products, on = 'product_id', indicator = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d255bdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_products_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95bbe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and display rows where the value in the '_merge' column is 'left_only'\n",
    "\n",
    "df_filtered = df_orders_products_merged[df_orders_products_merged['_merge'] == 'left_only']\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d62618",
   "metadata": {},
   "source": [
    "#### Based on the information from the dataframe above (filtered_df), we are only going to want to look at\n",
    "#### the rows that have complete information. If an order_id does not have a product_id associated with it, we \n",
    "#### do not want to include it in our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673afa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see how many types of each merge there were\n",
    "\n",
    "df_orders_products_merged['_merge'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7ce28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I ran a value_counts() function after running each type of join. I just copied and pasted the results below\n",
    "# so that I had a reference of them. \n",
    "\n",
    "# Checking to see how many types of each merge there were with the inner join\n",
    "\n",
    "# df_orders_products_merged['_merge'].value_counts()\n",
    "\n",
    "# both          32404859\n",
    "# left_only            0\n",
    "# right_only           0\n",
    "# Name: _merge, dtype: int64\n",
    "\n",
    "\n",
    "# Checking to see how many types of each merge there were with the outer join\n",
    "\n",
    "\n",
    "# df_orders_products_merged['_merge'].value_counts()\n",
    "\n",
    "# both          32404859\n",
    "# left_only        30200\n",
    "# right_only          11\n",
    "# Name: _merge, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1734794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_products_merged.to_pickle(os.path.join(path, '02. Data', 'Prepared Data', 'orders_products_merged.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bdffbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
